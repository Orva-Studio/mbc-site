---
title: "I pledge allegiance to Anthropic no more"
description: "Why I think it's time everyone should stop using Anthropic models"
date: "2025-09-14"
author: "Richard Oliver Bray"
---

<img src="/images/blog/allegance.png" alt="Book cover" className="w-full mb-8" />

There's no doubt about it; when it comes to AI coding, **Anthropic models have always been ahead**. 

Ever since I used Claude Sonnet 3.5 in Cursor, I have been amazed by the results. This isn't just an experience that was unique to me; many people on the internet and even work colleagues have had the same.Â 

This fact has been so true that whenever a new coding-focused model is released by a company that isn't Anthropic, everyone asks the question: **Is it as good as Claude?** Typically, the answer is no. Both Sonnet 3.7 and Sonnet 4 have maintained Anthropic's stronghold; however, the recent release of models like [Kimi K2](https://moonshotai.github.io/Kimi-K2/), [GLM 4.5](https://z.ai/blog/glm-4.5), [GPT-5](https://openai.com/index/introducing-gpt-5/), and the reduced price of these models has started to **waver my allegiance to Anthropic models**.

Before continuing, I want to get one thing off my chest. I predominantly use Sonnet and rarely use Opus. **Sonnet has always been enough for me**. That being said, I'm aware that not using any Opus models in a blind spot on my ability to judge all Claude models fully, but hopefully the points I raise in this post will still be valid despite that.

## Claude is no longer far ahead

Despite what you think about benchmark scores for AI models, they're the best way at a glance to judge a model's performance. Without, of course, trying the model yourself. I don't want to go into detail on all the coding benchmarks, but I personally think **SWE-bench** and **LiveCodeBench** are very good at telling if a model is good at real-world coding tasks. So if we take a look at scores for SWE-bench, remember that higher is better.

| Model     | Score  |
|-----------|--------|
| Opus      | 67.60  |
| GPT-5     | 65     |
| Sonnet    | 64.93  |
| GPT-5 mini| 59.80  |
| o3        | 58.40  |
| Qwen3     | 55.40  |

Note: These scores are from SWE-bench verified using the [mini-SWE-agent](https://github.com/SWE-agent/mini-swe-agent).

Also, (yes, this is still part of the note), GLM 4.5 claims to have a score of 64.2 using the [OpenHands agent](https://github.com/All-Hands-AI/OpenHands), which is better than the mini-SWE agent in my opinion. Kimi-K2 0711 gets a score of 43.80, but the newer one claims to have a score of 69.2. I believe that number is inflated since it's not from the SWE-Bench leaderboard site, but I'll let you make your own judgement.
